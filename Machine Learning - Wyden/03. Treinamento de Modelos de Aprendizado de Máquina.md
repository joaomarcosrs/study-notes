# Descrever o treinamento do modelo de máquinas de vetores de suporte

**SVM (Support Vector Machine)** é uma técnica apresentada por Vapnik et al. (1995) ==como uma abordagem geométrica para o problema de classificação==. Cada elemento do conjunto de observações é um ponto Pi num espaço S, e o ==aprendizado consiste em separar os elementos positivos dos negativos== (classificação um contra o resto), ==ou separar os elementos em grupos de classes== (multiclassificação um a um), ==por meio da otimização dos limites de decisão== (vetores de suporte).

*O algoritmo SVM funciona da seguinte forma:* 
1. Imaginemos o espaço amostral, em que podemos observar elementos de duas classes distintas.

![[1.jpg]]

2. Para tentarmos separá-las, vamos traçar um vetor aleatório no espaço, em que ainda não conseguimos o resultado esperado.

![[2.jpg]]

3. Vamos instanciar mais vetores; o algoritmo do SVM tenta maximizar as margens entre as distâncias mais próximas. 

![[3.jpg]]

4. Assim, podemos observar o estabelecimento dos nossos vetores de suporte (agora em pontilhado) e o vetor de decisão (agora em linha contínua). Ao final, a nova instância é classificada como um losango.
![[4.jpg]]

Olhando de forma mais matemática, conforme Santos (2002), imagine um conjunto de exemplos E, onde cada observação seja um par ordenado:

$$
(X_i,Y_i)|i = [1,2,...,n]
$$

Onde $X_{i} \in R^{M}$ ==é uma representação vetorial de uma observação==, e $Y{i} \in {0,1}$ ==seria sua classe associada==. Imagine que exista uma distribuição de probabilidade $P(x,y)$ da qual os dados de exemplos foram extraídos, mas que não conhecemos. Durante o processo de treinamento, ==o classificador deverá aprender a fazer o mapeamento do conjunto de observações às suas classes $(X \rightarrow Y)$, de modo que o mesmo seja capaz de classificar uma nova instância inédita==. Assim, o classificador SVM deverá minimizar a expectativa de erro $E(\alpha)$ em uma classificação, tal que $E(\alpha)$ é dado por:
$$
E(\alpha) = \\\int_{}^{} \frac{1}{2} \mid Y - f(X,\alpha)\mid \, dP(X,Y)\space\space\space\space\space\space\space\space (1.1)
$$
Note que $P(X,Y)$ é desconhecido, logo não seria possível calcular essa equação. Por outro lado, a função de Erro Empírico, $E_{emp}(\alpha)$, ==é definida como a média da taxa de erro nos elementos do conjunto de exemplos== e pode ser definida como:
$$
E_{emp}(\alpha) = \frac{1}{2}n\sum_{i=1} ^{n} \mid Y_{i} - f(X_{i},\alpha) \mid \space\space\space\space\space\space\space\space (1.2)
$$
É válido observar que o $E_{emp}$ é fixo para um valor arbitrário de alpha $(\alpha)$ e um conjunto de exemplos ${X_{i},Y_{i}}$.

Para entendermos melhor a matemática a seguir, apresentaremos o conceito de Dimensão Vapnik-Chervonenkis (Dimensão VC). ==Esta é uma medida de capacidade (complexidade, riqueza e flexibilidade) de um conjunto de funções que podem ser aprendidas por um algoritmo estatístico binário de classificação==.

Dito isso, seja $h$ a dimensão-VC do conjunto ${f,(\alpha)}$, e seja $E_{emp}(\alpha)$ definido pela equação 1.2. Ao selecionarmos um $k$ tal que $0 \leq k \leq 1$ , o seguinte limite é válido com probabilidade de pelo menos $1 - k$ para $n > h$:

$$
E(\alpha) \leq E_{emp}(\alpha) + \sqrt{\frac{h\log[\frac{20}{h}]-\log(\frac{k}{4})}{n}}\space\space\space\space\space\space\space\space\space\space\space\space\space  (1.3)
$$

O termo somado a $E(\alpha)$ (componente em raiz quadrada) é conhecido como **"termo de complexidade"**. Assim, ==minimizando o lado direito da equação 1.3, podemos encontrar a melhor máquina de vetores de suporte que atenda à tarefa proposta de classificação==.

## Tarefas de Aprendizado de Máquina

No âmbito de Aprendizado de Máquina, temos o que chamamos de tarefas de aprendizado. Algo semelhante ocorre com um ser humano, que, nos seus primeiros anos de vida, precisa aprender a encontrar significados para o que ele entende, seja por similaridade, por experiência/vivência ou por ser um axioma, isto é, a classificação. Ele pode, ==a partir de características das observações feitas, calcular e projetar ações/perspectivas, o que podemos entender como regressão==, e, finalmente, ==juntar observações em um compartimento físico ou mental devido à proximidade física ou conceitual== (similaridade). Chamamos isso de agrupamento (**clusterização**).

Ao visualizarmos nossos dados, antes de partirmos para o treinamento de modelos, a clusterização é uma etapa muito importante, ==principalmente para classificadores que dependem da distribuição geométrica dos dados de treinamento==, como o SVM. Assim, saberemos qual seria o tipo de classificação mais adequada, linear ou não linear, pois é possível separarmos nossos dados a partir de um hiperplano linear ou não, como podemos ver nos exemplos abaixo.

![[5.jpg]]![[6.jpg]]

Na **classificação linear**, ==o hiperplano calculado pela máquina de vetores de suporte vai ter a função de erro a ser otimizada de forma muito parecida com a equação da reta que conhecemos== com a seguinte forma, definindo, respectivamente, os limites inferior e superior:
$$
<X(n),w> + b < 0e < X(n),w > + b \geq 0
$$
Já na **classificação não linear**, vamos observar ligeira mudança de comportamento. Como o espaço não é linearmente separável, entre as observações, ==as funções de decisão terão um comportamento similar aos algoritmos de vizinhos mais próximos, onde se deseja minimizar a distância entre elementos da mesma classe e maximizar a distância entre os conjuntos de diferentes classes==.

Chamamos de **aprendizado de função de kernel** a subtarefa de apreender essa função não linear, que é a mais comum e genérica, ==justamente por tornar este comportamento parecido com os vizinhos mais próximos==, sendo a função baseada em **radial** (RBF).

Já para a tarefa de regressão, o objetivo do classificador é ==projetar uma função que seja o mais semelhante possível à equação da reta da classe numérica desejada==.

## Cotidiano do SVM
Usamos muito o SVM para tarefas de classificação, principalmente de ==conjuntos de dados cuja separação geométrica é evidente==, como, por exemplo, imagens, mapas, grupos polarizados etc., ==ou para conjuntos de dados cuja distribuição é complexa e tem _overlap_ de grupos==, como, por exemplo, categorias de textos.

* #### Vantagem do SVM
	É a ==eficácia de trabalhar em espaços dimensionais muito grandes==, podendo ser usado mesmo quando a quantidade de dimensões é maior do que a quantidade de amostras. ==Ele utiliza um subconjunto de pontos de treinamento na função de decisão== (**vetores de suportes**) ==e é versátil, pois permite que outras funções de kernel possam ser utilizadas para calcular os limites de decisão==.

* #### Desvantagem do SVM
	É o ==custo computacional para fornecer probabilidades de predição==, uma vez que isso não é natural do algoritmo por trás dele nem de sua concepção, além de tomar toma muito tempo de treinamento para conjuntos de dados muito grandes. ==Devido ao fato de ser um classificador geométrico, sofre bastante em virtude do excesso de características no _dataset_==. Quando isso acontece, são necessárias regularizações e configuração de parâmetros para não ocorrer superajuste de modelo (_overfitting_).
# Descrever o treinamento do modelo de árvores de decisão
É um **classificador** construído de cima (raiz) para baixo (folhas). Na **raiz**, são alocados todos os exemplos de treinamento; logo, ==eles são agrupados pelo atributo de melhor separação do conjunto inicial== (escolhido de forma iterativa). Escolhido o atributo, ==são criados N nós filhos da raiz para os possíveis valores desse atributo, e os elementos que estavam na raiz são alocados para esses filhos de acordo com a condição do valor do atributo escolhido==.

![[9.jpg]]

De forma ==recursiva para cada nó interno, fazemos o mesmo processo==, ou seja, escolhemos o atributo mais adequado, agrupamos os dados, criamos filhos para cada valor possível e os dados são alocados nos nós filhos, ==até que o nó filho não possa mais ser partido em outros agrupamentos==.

> Tal algoritmo é muito versátil, pois atende a qualquer tipo de dado, seja categórico, seja numérico (diferentemente da maioria dos modelos que exigem pelo menos a transformação dos dados categóricos em numéricos). Devido à sua simplicidade e indução compreensível, é muitas vezes preferido em detrimento de classificadores mais complexos, e, por trabalhar com a estrutura de dados de árvore, é assistido por diversas técnicas de Análise de Algoritmos para Árvores, como busca binária, poda de ramos, balanceamento etc.

## Treinando e visualizando a árvore de decisão
O processo de treinamento da maioria dos modelos de Aprendizado de Máquina, incluindo árvore de decisão, ==baliza-se na otimização de uma função de custo para que o modelo evolua e aprenda==.

No caso das **árvores de decisão**, elas dependem de algumas **funções de custo**. Uma delas ==é a que determina qual é o melhor atributo a ser escolhido para dividir o conjunto de dados==. Essa função se baseia no conceito de **entropia**, ==que caracteriza a impureza dos dados em seu conjunto, ou seja, a falta de homogeneidade dos dados de entrada em relação à sua classificação== — **quanto maior a entropia, mais heterogênea é a amostra** (MITCHELL, 1997). Dado um conjunto $S$, de classes $i$ com proporções $p$, a entropia é expressa pela fórmula:

$$
Entropia(S) = \sum p_{i}\log_{2}(p_{i}) \space\space\space\space (2.1)
$$

A outra função de custo é a de **Ganho de Informação**, ==que depende do conceito de entropia== que apresentamos anteriormente, pois é dada em função do mesmo. O ganho de informação é dado pela seguinte fórmula:
$$
Ganho(S,A) = Entropia(S) - \sum_{v \in valores(A)} \frac{\mid S_{v}\mid}{\mid S \mid} Entropia(S_{v}) \space\space\space\space(2.2)
$$

Vamos interpretar a **fórmula 2.2**, onde temos $A$ como atributo analisado, $v$ como possíveis valores do atributo $A$, $S$ como o conjunto de todos os atributos e $S_{v}$ como o subconjunto de $S$, em que $A = v$. Logo, ==a otimização para o aprendizado da árvore é maximizar o ganho de informação a cada etapa de construção da árvore==.

> A árvore de decisão pode ser também empregada para a **regressão**.

==A diferença está no tipo de variável que é a classe e nas funções que ela vai otimizar para ajustar o modelo à distribuição de probabilidade da classe==. A classe passa a ser uma variável numérica, no domínio dos Reais, enquanto a função a ser otimizada passa a ser a do **Mínimo Erro Quadrático**, ou, em alguns casos, a função a ser otimizada pode ser a **Desvio de Meia Poisson**. ==Árvores de decisão usadas para problemas de regressão são chamadas também de árvores de regressão, onde cada nó terminal ou folha contém uma constante (geralmente, uma média) ou uma equação para o valor previsto de determinado conjunto de dados==.

## Identificando e configurando os hiperparâmetros da árvore de decisão
==Algoritmos baseados em árvores herdam os benefícios das estruturas de dados que lhes dão nome, mas também herdam suas peculiaridades e limitações==, como, por exemplo, necessidade de balanceamento para otimização de custo computacional.
* ### Hiperparâmetros
	São atributos dos modelos de classificação ou regressão que, ao serem regulados, podem causar impacto no processo de aprendizado e trazer resultado do algoritmo de Aprendizado de Máquina em questão. No caso das árvores de decisão, ==os principais hiperparâmetros da árvore de decisão são sua altura (ou profundidade) e o número máximo de características a serem analisadas por nós internos e número mínimo de folhas==.

A **altura da árvore** (ou **profundidade máxima**) ==é o parâmetro que dita a regra de expansão da árvore: se deixado nulo, a árvore expandirá até que as folhas estejam puras ou tenham a quantidade de amostras mínimas especificadas por folha; ou, se atribuído um valor inteiro, a árvore para sua ramificação até determinada altura e considera os nós dessa camada como folhas==.

> Quanto maior a profundidade da árvore, maior é a chance de **sobreajuste** do conjunto de treinamento.

O número máximo de características a ser analisado por nó interno ==é uma restrição aplicada ao conjunto de características pelo qual o processo de escolha de melhor divisor pode optar==, ou seja, de maneira forçada, ==dizemos para o algoritmo olhar para uma fração ou uma quantidade inteira de características e, dentre elas, escolher o melhor divisor de agrupamento==. Essa restrição também é aplicada como forma de regularização do algoritmo, uma vez que conjuntos muito grandes de características podem causar sobreajuste do modelo. Contudo, lembrando que, ==caso o processo de decisão não consiga achar um candidato divisor válido, o algoritmo ignora a restrição e volta a olhar para todas as características até encontrar um divisor válido para aquela etapa==.

O **número máximo de folhas** é um parâmetro que limita a quantidade de folhas que cada ramo pode ter. ==Este é mais um parâmetro de regularização da árvore, de modo que o modelo não fique com poucos nós de decisão (menos ramos) em prol de uma copa maior (mais folhas num só ramo), o que poderia causar desbalanceamento na árvore e deixá-la ineficiente para predições que caíssem de determinado "lado"==. Dessa forma, ao restringir a copa, escolhendo, por exemplo, duas folhas no máximo, temos uma árvore com mais decisões (mais robusta) e equilibrada.

#### Comentário
> Ainda sobre as folhas, ==temos o tamanho mínimo da amostra que restringe ou flexibiliza a quantidade de amostras de valores da classe tolerável de coexistir num nó folha==. Se considerado o padrão 1, a folha deve ser pura de determinado valor da classe; caso contrário, se escolhermos um número real, por exemplo, podemos ter proporções de observações da classe numa só folha, e, se tivermos valores inteiros, então, considera-se a quantidade mesmo. Tal regularização permite a flexibilização do modelo, podendo tolerar outliers mais facilmente.

==O processo de configuração de hiperparâmetros não deve ser feito de forma arbitrária==, apenas em caso de aprendizado, pois, ao construirmos um modelo de Aprendizado de Máquina, queremos otimizá-lo de forma sistemática, metodológica e replicável. Dito isso, ==o processo correto de escolha de configurações deve ser feito de forma iterativa, por meio de variação dos valores configurados e monitorando o resultado de métricas. Ao fim deste processo, escolhemos a combinação de configurações utilizadas que maximizam o resultado de métricas de avaliação.==
# Descrever o treinamento do modelo de florestas aleatórias
## Compreendendo modelos incorporados

> O **aprendizado incorporado** ==é a estratégia de aprendizado na qual se toma vantagem da variabilidade de conhecimentos que uma população tem em relação a somente um indivíduo==.

Entre as técnicas mais usadas, temos a **votação** e o **empilhamento**.

* ### Votação
	==Consiste no agrupamento de três ou mais algoritmos de aprendizagem por tarefa de aprendizado== (**classificação**, **regressão** ou **clusterização**) ==e estes treinam sobre o mesmo _dataset_==. Então, quando uma nova observação é apresentada ao comitê de modelos treinados, cada integrante emite um parecer, e ocorre uma votação (normal ou ponderada) em relação aos pareceres, da qual sairá o resultado final. O método está ilustrado na figura abaixo:
	
	![[13.jpg]]

* ### Empilhamento
	O processo é parecido com o que o ocorre em redes neurais, em que ==há transformação do conjunto de treinamento à medida que ele vai sendo transformado durante o avanço do aprendizado==. Ou seja, ==no caso do aprendizado empilhado, cada modelo do comitê recebe como input o resultado dos modelos anteriores e se chega a uma conclusão==. Por exemplo, imagine um modelo não supervisionado que recebe o _dataset_ inicial, descobre-se automaticamente agrupamentos escondidos nele, devolve-o acrescido dessas colunas e passa-o à frente para um classificador de árvore de decisão. Nesse caso, temos um modelo empilhado híbrido de aprendizado não supervisionado e aprendizado supervisionado para uma tarefa de classificação, como ilustra a figura abaixo:
	
	![[14.jpg]]

Existe também o método de incorporação **bootstrap aggregating**, que, ==além de envolver o uso de um comitê de modelos, ainda faz a randomização das amostras de distribuição dos dados entregues a cada um dos modelos membros do comitê==. É justamente sobre ele que vamos falar mais à frente com as **florestas aleatórias**, **Random Forests**, demonstradas na figura abaixo:

![[15.jpg]]

## Treinamento da Floresta Aleatória

Inicialmente propostas por Breiman (2001), as florestas aleatórias, ==em um primeiro momento, foram criadas para regularizar um problema de árvores de decisão individuais que cresciam muito, ou seja, aumentavam indiscriminadamente seus nós internos e iam aprendendo padrões irregulares que não deviam, causando **sobreajuste**, diminuindo o viés e **aumentando demais a variância**==. Muito semelhante ao fenômeno de estiolamento de plantas que são parcialmente ou totalmente privadas da luz solar e precisam se esticar até achar uma brecha de luz, deformando-se no processo.

O aprendizado da floresta aleatória se dá da seguinte forma para um conjunto de treinamento:
$$
X_{i} = X_{1},X_{2},\dots,X_{n-1}, X_{n}
$$

E um conjunto de classificações:
$$
Y_{i} = Y_{1},Y_{2},\dots,Y_{n-1}, Y_{n}
$$

==O algoritmo de aprendizado das florestas aleatórias começa a sortear amostras desses conjuntos com substituição==, a fim de formar **conjuntos aleatórios** que são "fotos" de partes do conjunto de dados inicial. Feito isso, ==cada árvore de decisão na floresta recebe esse pacote de amostras e treina utilizando o mesmo algoritmo das árvores de decisão==.

Uma vez realizado o treinamento, ==quando um novo elemento for apresentado para a floresta, as árvores avaliam esse novo elemento, e cada uma emite um parecer sobre ele, ou emite o resultado de um cálculo em caso de regressão==. Visto que todas as árvores terminaram sua análise e emissão de pareceres, ==o conselho/comitê da floresta decide o resultado final a partir de uma votação==, no caso de problemas de **classificação**, ou média, no caso de um problema de **regressão**, como visto na figura abaixo.

![[16.jpg]]
> Treinamento de Florestas Aleatórias: 
> 1 - Definição do estimador base e dados; 
> 2 - Replicar a classe básica; 
> 3 - Fazer amostras dos dados originais; 
> 4 - Treinamento da Floresta; 
> 5 - Apresentação de Nova Instância e Votação; 
> 6 - Emissão de Parecer Votado.

Ao mesmo tempo em que as florestas aleatórias são muito eficientes em prover uma decisão (classificação ou regressão) mais justa e confiável pela definição de seu algoritmo, temos a contrapartida: a falta de explicabilidade (_explainability_).

#### Explicabilidade
> É uma disciplina da Inteligência Artificial que tem como compromisso ==fazer a prova real do processo, algoritmo ou heurística de aprendizado envolvido em um modelo, explicando para o humano que está interagindo com o computador como uma decisão importante foi tomada==, como, por exemplo, sobre concessão de crédito. Em árvores de decisão, a explicabilidade é evidente. ==Basta seguir o caminho da raiz até a folha de classificação, e assim entendemos como o modelo raciocinou sua escolha==.

No caso das florestas randômicas, ==é muito mais complicado poder acompanhar esse caminho decisório, pois seria necessário iterar por cada membro da floresta, extrair o caminho decisório e, a fim de extrair todos os caminhos, teríamos de descobrir (caso a implementação não nos disponibilize) o critério do comitê: se foi votação simples, votação ponderada etc==. Porém, tirando a explicabilidade, existem cuidados a serem tomados pelos usuários das florestas aleatórias, como veremos na próxima seção.
#### Comentário
> Existe ainda um modelo variante das florestas aleatórias, que é o **Extra Trees**. Este modelo é similar às florestas aleatórias, porém, ==em vez de implementar a estratégia incorporada de _bagging_, repassa o conjunto completo para as árvores de suas florestas==. O outro diferencial é que, ==na hora de decidir o ponto de corte nos nós internos da árvore, em vez de ocorrer a escolha do melhor atributo que divide os dados segundo o critério de separação pelo ganho de informação, as árvores extras fazem isso de forma aleatória==.

## Identificando e configurando os hiperparâmetros da Floresta Aleatória
Assim como a árvore de decisão, ==a floresta aleatória tem parâmetros parecidos, que regulam o que chamamos de estimador base, a classe que será "plantada" pela floresta==. Então, aqui podemos considerar os mesmos **hiperparâmetros** da árvore de decisão.

Além dos hiperparâmetros herdados da árvore de decisão, a floresta conta com o hiperparâmetro de **número de estimadores**, ==que define basicamente quão densa é a floresta==. É muito **importante** tal parâmetro, pois se, naqueles herdados da árvore básica, nós queremos regularizar a altura da árvore, o número de folhas etc., ou seja, topear a árvore, garantindo que não irá superajustar o conjunto de treinamento, ==é por meio da quantidade de árvores na floresta que enaltecemos a grandeza desse modelo==.

==Quanto maior o número de árvores, mais variado é o comitê, menor a chance de não termos predições viciadas, regularizando, assim, o aspecto de viés e deixando a floresta mais genérica (não terá grandes dificuldades para predizer elementos completamente inéditos)==. Claro que isso deve ser feito com o devido **cuidado**, ==pois uma floresta excessivamente grande pode se tornar computacionalmente muito mais custosa do que deveria e, devido ao excesso de variedade, pode não convergir em votação==.

1. Sempre que fizermos configurações de **hiperparâmetros**, precisamos fazer isso de forma sistemática, ou seja, utilizaremos o **método de grid search**, no qual ==preparamos uma matriz onde cada coluna é um parâmetro e cada linha é preenchida com combinações de valores dessas colunas==.

2. Após isso, iteramos sobre essas configurações linha a linha e, com **validação cruzada**, treinamos nossos modelos.

3. A cada resultado, normalmente, **métricas de performance**, como ==acurácia, precisão medida F, tempo de treino, são anotadas em colunas adicionadas nesta matriz à medida que os treinamentos acabam==.

4. E, no fim, selecionamos a configuração que resultou no **maior valor de métrica(s) selecionada(s)**.

De modo geral, em comparação com outros modelos, ==as florestas aleatórias tendem a alcançar melhor resultado em métricas, como acurácia, medida F, curva ROC, devido à estratégia de incorporação inerente de sua arquitetura==. Contudo, é muito importante ==executarmos sempre a configuração de hiperparâmetros==, pois, assim, não só tentaremos ==reduzir a chance de sobreajuste==, mas também seremos **transparentes** em nosso processo experimental, uma vez que os modelos incorporados, naturalmente, têm sua explicabilidade prejudicada.
# Descrever o treinamento do modelo de redução de dimensionalidade

## Conceito de redução de dimensionalidade

> Dimensionalidade é o número de características (**features**) de um conjunto de dados.

As principais razões para diminuir a dimensionalidade são: ==diminuir o custo (tempo e processamento) de medição dos modelos e melhorar (ou, pelo menos, não prejudicar) a precisão do classificador==. Se o modelo tiver de se preocupar só com as características principais, ele ocupará menos memória e poderá processar os dados muito mais rapidamente. Dessa maneira, é mais fácil o trabalho dele.

É importante salientar que nem sempre conseguimos conjuntos muito grandes de dados para trabalhar (seja por falta de qualidade, acesso ou, simplesmente, novidade); então, **para esse cenário**, ==é preferível a escolha de um número menor de dimensões, evitando redundâncias, dispensando informação inútil== etc.

É importante retomar o conceito de **maldição da dimensionalidade**, que está muito relacionado ao assunto, por se tratar de uma situação crítica para o desempenho dos modelos. Assim, a maldição da dimensionalidade (**Dimensionality Curse**) ==é o excesso de variáveis em um problema (em nosso caso, excesso de características em um problema de Aprendizado de Máquina)==.

#### Atenção
> Segundo Watanabe (1985), ==é possível fazer dois padrões arbitrários se tornarem similares a partir da codificação de características similares==. Para a redução de dimensionalidade, temos duas alternativas: a **extração de características** e a **seleção de características**.

* ### Extração de características
	==Este algoritmo combina as características originais em um conjunto reduzido de nova(s) característica(s)==. A extração de características, por mais que seja melhor para o modelo, ==acaba deformando os dados no sentido em que eles não fiquem tão explicáveis ou inteligíveis para nós, perdendo o seu significado físico/material==

* ### Seleção de características
	Este algoritmo, segundo um conjunto de regras e objetivos, ==escolhe as melhores características em detrimento das demais==. A seleção de características, normalmente, ==reduz o custo da medição de dados== (mentalmente imagine uma expressão polinomial gigante se reduzindo a apenas 2 ou 3 termos). Porém, ==esse tipo de redução não deforma o dado, apenas destaca alguns aspectos e ofusca outros para facilitar o processamento, bem como para melhorar resultados==.

> O mais importante de tudo: o excesso de redução é perigoso para os modelos, pois ele pode ficar muito ingênuo e julgar resultados por aspectos rasos e/ou desconexos.

## Análise de Componentes Principais (PCA)

> ==É um método de redução de dimensionalidade do tipo extrator de características==. A popularidade da PCA se deve ao fato de ser simples, rápida, linear e não paramétrica.

Tal método é baseado na **variância dos dados**, tentando ==criar uma nova representação (combinação) destes, normalmente em uma dimensão menor, mantendo a variância entre eles==. Imaginando um plano bidimensional cartesiano, cada eixo possuiria um percentual de variância individual com relação ao todo, e seriam completamente desconexos entre eles.

Matematicamente, a **PCA** é definida como ==uma transformação linear ortogonal que transporta os dados para um novo sistema de coordenadas, onde a variância é distribuída ordenadamente em grandeza pelas novas coordenadas==, ou seja, a maior variância é passada para a primeira coordenada nova, a segunda maior variância para a segunda variável nova, e assim por diante. Normalmente, para fins de visualização, limitamos a quantidade de novas variáveis a apenas três, ainda que não seja metodologicamente o certo (que seria um processo similar ao de configuração de hiperparâmetros, ou seja, ==teste iterativo de parâmetros objetivando a maximização da métrica do modelo==).

Imaginemos uma matriz de dados, $M^{T}$, com média amostral nula. Cada linha $n$ é uma observação, e cada coluna $m$ seria uma característica. Agora, decomponham essa matriz em outras três, $W$, $\Sigma$, e $V^{T}$, de tal forma que:
$$
M^{T} = W\Sigma V^{T}
$$

Onde, 
$W$: é a matriz de covariância $MM^{T}$ com dimensões $m$ por $m$.
$\Sigma$: é a matriz diagonal $m$ por $n$ retangular com números reais não negativos nesta diagonal. $V^{T}$: é a matriz de dimensões $n$ por $n$ dos autovetores de $M^{T}M$. Então, a transformação que mantém a mesma dimensionalidade é dada por:
$$
C^{T} = M^{T}W \space\space\space\space
$$
$$
C^{T} = V\Sigma^{T}W^{T}W \space\space\space\space (4.1)
$$
$$
C^{T} = V\Sigma^{T}
$$

Como $W$ é uma matriz ortogonal graças à definição da **Decomposição de Valores Singulares** de uma Matriz Real da Álgebra Linear, e cada linha de $C^{T}$ é apenas uma rotação da linha correspondente em $M^{T}$, então, a primeira coluna de $C^{T}$ é formada a partir dos resultados relativos à primeira componente, a segunda coluna de $C^{T}$ é formada a partir dos resultados relativos à segunda componente, e assim por diante. Dessa forma, ==para a redução da dimensionalidade, basta projetar as observações ao espaço reduzido definido pelos primeiros $X$ vetores da matriz $W_{X}$ da seguinte forma==:
$$
C = W_{x}^{T}M = \Sigma_{X}V^{T} \mid \Sigma_{X} = I_{X,m} \space\space\space\space (4.2)
$$
Essencialmente, a **PCA** ==faz uma rotação dos pontos no espaço dimensional em torno da média, tentando alinhá-los aos componentes principais escolhidos a partir da minimização das distâncias quadradas a essa média==. Desse jeito, essa operação de rotação deixa os primeiros componentes principais mais variados do que os últimos, podendo descartá-los por não terem tanto valor como informação. Assim, ==ela é a melhor transformação ortogonal para manter o menor subespaço mais variado; contudo, o custo computacional dessa operação é muito maior à medida que se tem muitas dimensões==.
#### Atenção
> Para que a **PCA** funcione de forma "justa", ==as componentes devem ter a mesma escala==, senão, falando em alto nível, uma componente poderia absorver a outra devido à ordem de grandeza (por isso a normalização dos dados é tão importante num processo de Aprendizado de Máquina).

* ### Vantagem
	A **PCA** ==tem como vantagem a simplicidade do seu algoritmo devido ao fato de ser linear==. É um algoritmo disponível em quase todas as bibliotecas de Aprendizado de Máquina, sendo altamente explicável.

* ### Desvantagem
	A **PCA** ==tem como desvantagens a sensibilidade à escala dos componentes analisados, o custo computacional, que é muito grande para conjuntos grandes, e a dificuldade de lidar com dados distribuídos de forma não linear no espaço geométrico de observações==.
## Redução de Dimensionalidade Não Linear através da Incorporação Linear Local (LLE)
Agora, no aspecto de **problemas não lineares**, ou **redução de dimensionalidade não linear** (NLDR), temos a **Incorporação Linear Local (LLE)**. A LLE ==tem vantagens sobre algoritmos que lidam com matrizes esparsas, bem como apresenta melhores resultados em problemas não lineares do que outras técnicas como a Kernel PCA==.

Como o nome implica, a LLE tem o funcionamento parecido com os algoritmos de vizinhos mais próximos, no sentido de que ==cada ponto (observação) no espaço dimensional é descrito por meio do cálculo de pesos que representam tal ponto em função dos seus vizinhos localmente próximos, como uma combinação linear==. Quando todas as observações forem mapeadas, o algoritmo, por meio da aplicação da **técnica de otimização de autovetores**, ==acha as representações com menor dimensão compatível com o mapeamento de seus vizinhos==.

O erro de reconstrução das observações é dado pela seguinte **função de custo** $E(W)$:
$$
E(W) = \Sigma_{i} \mid X_{i} - \Sigma_{j}W_{ij}X_{j} \mid^{2} \space\space\space\space (4.3)
$$

==Os pesos descritos pela matriz $W_{ij}$ são os valores de contribuição que o ponto $X_{j}$ tem, enquanto o ponto $X_{i}$ é reconstruído==. Essa **função de custo**, dada pela equação 4.3, é otimizada segundo **duas restrições**: ==cada ponto $X_{i}$ só pode ser reconstruído a partir de seus vizinhos, reforçando que $W_{ij}$ seria $0$ se o ponto $X_{j}$ não fosse vizinho de $X_{i}$==; e o ==somatório de pesos de cada linha da matriz $W_{ij}$ seria igual a $1$, ou $\Sigma_{j} W_{ij} = 1$==.

Estabelecido o mapeamento dos pesos em $D$, precisamos mapeá-los em uma dimensão $d$, menor. Para tal, teremos de minimizar a função de custo:
$$
E(Y) = \Sigma_{i} \mid Y_{i} - \Sigma_{j}W_{ij}X_{j} \mid^{2} \space\space\space\space (4.4)
$$

Lembramos que, na maioria dos casos (projetos) em que um cientista de dados ou engenheiro de modelos trabalha, não é necessária toda essa matemática, mas é importante para compreendermos a complexidade por trás da decisão de utilizarmos um redutor de dimensionalidade e nos dar a ideia, por exemplo, de que, **para esse caso**, o algoritmo de LLE irá: 
1. Mapear todos os dados originais.
2. Calcular a distância entre eles num raio de vizinhos próximos.
3. Traduzir esse mapa em função desse mapeamento de vizinhos.
4. Reconstruir o mesmo mapeamento numa dimensão menor, mantendo essas proporções.

O algoritmo de LLE usa, normalmente, ==a distância euclidiana para calcular os pesos da matriz de vizinhanças $W$ e tem um parâmetro livre $K$, que estipula justamente a quantidade de vizinhos a serem observados==, lembrando que o $K$ pode ser descoberto a partir do mesmo processo de configuração de hiperparâmetros que vimos anteriormente neste módulo. Quanto maior o número de dimensões e de dados, mais custoso será esse processo, sempre.
#### Comentário
> O LLE é um algoritmo poderoso, adaptável e apresenta boa generalização para resolver problemas não lineares de redução de dimensionalidade. Porém, assim como a grande maioria dos algoritmos não lineares, causa certa dificuldade no quesito explicabilidade.