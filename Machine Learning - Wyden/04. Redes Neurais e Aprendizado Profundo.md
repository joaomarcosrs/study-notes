# Reconhecer as etapas necessárias para fazer o treinamento de redes neurais profundas
## Problemas dos gradientes
Uma rede neural é formada por neurônios interconectados por meio de ligações chamadas de sinapses. Na imagem a seguir, podemos ver os elementos principais de uma rede neural.

![](https://raw.githubusercontent.com/joaomarcosrs/study-notes/main/Machine%20Learning%20-%20Wyden/static/img_04_01.jpg "Imagem: Sérgio Assunção Monteiro")

Nas sinapses, estão associadas entradas – representadas na imagem pelas variáveis $x_{1}$, $x_{2}$ e $x_{3}$, e – e pesos – representados pelas variáveis $w_{1}$, $w_{2}$ e $w_{3}$, e – combinados por meio de uma função que faz a multiplicação das entradas com os respectivos pesos e soma com o **bias**, obtendo, assim, $z_{j}$, conforme podemos ver na equação:

$$
z_{j} = \sum_{i=1}^Nx_{i}w_{ij} + bias_{j}
$$

> **Bias é uma constante que ajuda a Rede Neural, para que ela se ajuste melhor aos dados fornecidos. Ou seja, é uma constante que dá flexibilidade para um melhor desempenho da Rede Neural.**

A variável $z_{j}$ também é chamada de **função de propagação**. O índice “j” refere-se ao “j-ésimo” neurônio da rede. A partir da variável $z_{j}$, calculamos a chamada **função de ativação**, representada por $f(z_{j})$ na imagem. Essa função de ativação vai produzir $y_{j}$.
### Funções de ativação

Existem alguns tipos de funções de ativação. A seguir, apresentamos as mais comuns:
* #### Função linear
	A função linear e sua derivada são dadas, respectivamente, por:
	
	$$f(z) = \alpha z$$
	
	$$f'(z) = \alpha$$
	
	Na imagem, apresentamos o gráfico da função linear.
	
	![](https://raw.githubusercontent.com/joaomarcosrs/study-notes/main/Machine%20Learning%20-%20Wyden/static/img_04_02.jpg "Imagem: Sérgio Assunção Monteiro")

* #### Função sigmoide
	A função sigmoide ou logística e sua derivada são dadas, respectivamente, por:
	
	$$\sigma(z) = \frac{1}{1+e^{-z}}$$
	
	$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
	
	Na imagem, apresentamos o gráfico da função sigmoide.
	
	![](https://raw.githubusercontent.com/joaomarcosrs/study-notes/main/Machine%20Learning%20-%20Wyden/static/img_04_03.jpg "Imagem: Sérgio Assunção Monteiro")

* #### Função Tanh (tangente hiperbólica)
	A função tangente hiperbólica e sua derivada são dadas, respectivamente, por:
	
	$$\tanh(z) = 2\sigma(2z) - 1$$
	
	$$\tanh'(z) = 1 - \tanh²(z)$$
	
	Na imagem, apresentamos o gráfico da função tangente hiperbólica.
	
	![](https://raw.githubusercontent.com/joaomarcosrs/study-notes/main/Machine%20Learning%20-%20Wyden/static/img_04_04.jpg "Imagem: Sérgio Assunção Monteiro")

* #### Função ReLU (unidade linear retificada)
	A função ReLU e sua derivada são dadas, respectivamente, por:
	
	$$ReLU(s) = max\{0, s\}$$
	
	$$ReLU'(s) = \{\frac{1, \space se \space x \geq 0}{0, \space caso \space c.}\}$$
	
	Na imagem, apresentamos o gráfico da função ReLU.
	
	![](https://raw.githubusercontent.com/joaomarcosrs/study-notes/main/Machine%20Learning%20-%20Wyden/static/img_04_05.jpg "Imagem: Sérgio Assunção Monteiro")

A ideia central de uma rede neural é mapear uma entrada por meio de uma função de ativação. Dessa forma, a rede passa por um processo de treinamento que tem como objetivo determinar um conjunto de parâmetros – os **pesos sinápticos** – que produzem as melhores respostas.

Isso é obtido com a minimização da diferença entre o valor esperado e a saída do modelo. Por exemplo, vamos representar por “$\hat{y}$” o valor que esperamos que a rede neural obtenha para classificar determinada entrada e vamos representar por “$y$” a variável que a rede neural vai obter. A ideia, então, é minimizar o erro entre o valor obtido e o valor real. Essa função é dada por:

$$Erro(y) = \sum_{j=1}^N \frac{(y_{j}-\hat{y}_{j})^{2}}{2}$$

Onde $N$ representa a quantidade de unidades de saída.

Um dos algoritmos mais conhecidos para treinamento de redes neurais é o **backpropagation**, ou **algoritmo de retropropagação**. Esse algoritmo utiliza o método de gradiente de descida, que, por definição, compreende duas etapas:

	1. Calcular os gradientes da função de erro.
	2. Atualizar os parâmetros existentes em relação aos gradientes

As etapas 1 e 2 são repetidas até que o algoritmo obtenha o valor mínimo da função de erro.

Com um pouco mais de detalhes, o algoritmo de retropropagação pelo método de gradiente descendente pode ser formalizado como:

* ##### Entrada de dados
	$\space\space\space\space\space\space\space\space w, bias, x, \hat{y}, \eta$
	
* ##### Repita  
	$\space\space\space\space\space\space\space\space Calcule \space\space\space\space y_{j}$
	
	$\space\space\space\space\space\space\space\space Calcule \space\space\space\space Erro(y_{j})$
	
	$\space\space\space\space\space\space\space\space Calcule \space os \space gradientes\space\space\space\space \Delta w_{ji}$
	
	$\space\space\space\space\space\space\space\space Faça\space\space\space\space w_{ji} = w_{ji} - \eta\Delta w_{ji}$
	
	$\space\space\space\space\space\space\space\space Faça\space\space\space\space w_{ji} = w_{ji} - \eta\Delta w_{ji}$
	
* ##### Até que
	$\space\space\space\space\space\space\space Erro(y_{j}) < TOLERÂNCIA$

Onde $\eta$ é chamada de **taxa de aprendizado** e $\Delta w_{ji}$ é o **gradiente da função de erro** em relação ao **peso** $w_{ji}$.

**O gradiente de uma função corresponde à sua inclinação**. O processo de treinamento da rede neural com o uso de gradiente de descida reduz a função erro, aumentando, assim, o acerto da rede neural.

Infelizmente, existem alguns problemas quando aplicamos esse método. São eles (BENGIO; SIMARD; FRASCONI, 1994):

* #### Dissipação do gradiente (vanishing gradient)
	Trata-se de um cenário em que o modelo não aprende nada. Isso ocorre quando o gradiente se torna muito pequeno; implica que os pesos não mudarão para minimizar o erro.
* #### Explosão do gradiente
	Trata-se exatamente do oposto do desaparecimento do gradiente. O gradiente cresce excessivamente e o modelo nunca converge.
* #### Ponto de Sela (ponto minimax)
	O algoritmo fica estacionado em um ponto da função de erro que não permite sua minimização.

## Reutilização de camadas pré-treinadas

Na abordagem clássica de aprendizado de máquina, existem diferentes estratégias e técnicas de transferência de aprendizagem, que podem ser aplicadas com base no domínio, na tarefa em mãos e na disponibilidade de dados. Os métodos de aprendizagem por transferência podem ser categorizados das seguintes maneiras (PAN; YANG, 2010):

* #### Aprendizagem por transferência indutiva
	Nesse cenário, tanto o domínio da origem como o do destino são os mesmos, apesar de as tarefas de origem e destino serem diferentes umas das outras. O objetivo do método por indução é aprender com as instâncias de treinamento observadas e com um conjunto de suposições sobre a verdadeira distribuição dos casos de teste.

* #### Aprendizagem por transferência não supervisionada
	É semelhante à transferência indutiva com foco em tarefas não supervisionadas no domínio de destino. Os domínios de origem e destino são semelhantes, mas as tarefas são diferentes. Nesse cenário, os dados rotulados não estão disponíveis em nenhum dos domínios.

* #### Aprendizagem por transferência transdutiva
	Nesse cenário, existem semelhanças entre as tarefas de origem e de destino, mas os domínios correspondentes são diferentes. Nessa configuração, o domínio de origem tem muitos dados rotulados, enquanto o domínio de destino não tem nenhum. Isso pode ser classificado em subcategorias, referindo-se a configurações em que os espaços de características são diferentes ou a probabilidades marginais.

Uma evolução das redes neurais artificiais são as **redes neurais profundas (DNN)**, que incluem várias camadas entre as camadas de entrada e saída. Elas são usadas para diversas aplicações em que a abordagem tradicional não funciona bem, como processamento de imagens (visão computacional) e processamento de linguagem natural. Então, além da abordagem tradicional, temos os modelos de **transferência de aprendizado profundo (DTL)**. A DTL oferece uma flexibilidade muito maior em extrair recursos de alto nível e transferi-los de um problema de origem para um problema de destino. Entre as vantagens da DTL, temos:

1. Ajuda a resolver problemas complexos do mundo real com várias restrições.
2. Resolve problemas com poucos dados rotulados disponíveis.
3. Há transferência de conhecimento de um modelo para outro com base em domínios e tarefas.

Apesar das vantagens da transferência de aprendizado, existem questões importantes que devem ser tratadas como:

* #### Transferência negativa
	Há situações em que o processo de aprendizagem por transferência pode levar à queda no desempenho da rede neural. Isso ocorre em cenários em que a transferência de conhecimento da fonte para o destino não leva a nenhuma melhoria, mas causa uma queda no desempenho geral da tarefa de destino. Os motivos para a ocorrência desses problemas estão relacionados a casos em que a tarefa de origem não está suficientemente relacionada à tarefa de destino.

* #### Limites de transferência
	A qualidade da transferência deve ser quantificada para analisar qual foi o ganho real com esse processo.

## Otimizadores velozes

Treinar uma rede neural profunda de forma eficiente não é uma tarefa simples. Existem diversos algoritmos que são aplicados para o processo de otimização dos pesos de uma rede neural de aprendizado profundo. Os mais comuns são:

* #### Gradiente de descida (gradient descent)
	O método calcula o gradiente da função de custo associada em relação a cada peso $w_{k}$ e obtém o vetor gradiente $\Delta J(w_{k})$ usando a equação:
	
	$$w_{k+1} = w_{k} - \eta\Delta J(w_{k})$$
	
	Portanto, a velocidade do método depende exclusivamente do parâmetro de taxa de aprendizagem $\eta$.

* #### Gradiente de descida estocástico
	Em vez de calcular os gradientes para todos os seus exemplos de treinamento em cada passagem do método de gradiente de descida, ele usa apenas um subconjunto dos exemplos de treinamento de cada vez.

* #### Gradiente acelerado de Nesterov
	O método dá um “salto” com $w_{k} + \beta m_{k}$ na direção do gradiente acumulado anterior e depois mede o gradiente $\Delta J(w_{k} + \beta m_{k})$, onde termina e faz uma correção. A ideia é que é melhor corrigir um erro depois de cometê-lo. O esquema do algoritmo é dado pelas equações:
	$$m_{k+1} = \beta m_{k} - \eta\Delta J(w_{k}+\beta m_{k})$$
	
	$$w_{k+1} = w_{k} + m_{k+1}$$

Além desses métodos, também existem muitos outros, como AdaGrad, Momentum Optimization, RMSProp (root mean square prop), Adam e Nadam. O principal problema de usar um método de otimização inadequado é que o modelo leva muito tempo para convergir para um mínimo global ou fica preso em um mínimo local.

> A escolha de um otimizador vai acelerar a velocidade de treinamento e ajudar a melhorar o desempenho do modelo.
## Uso de regularização para evitar sobreajuste
**Overfitting** é um problema comum para todos os algoritmos de aprendizado de máquina. Ele ocorre quando o modelo se ajusta muito bem ao conjunto de treinamento, mas não tem um desempenho tão bom no conjunto de teste. Os modelos também podem apresentar o problema de **ajuste insuficiente**, que ocorre quando escolhemos um modelo muito simples que não funciona bem no conjunto de treinamento e teste. As formas mais comuns de reduzir o overfitting são por meio dos seguintes métodos:

* #### Regularizações L1 e L2
	Trata-se de adicionar um elemento extra à função de custo que penaliza o modelo por usar valores muito altos na matriz de peso. Desse modo, tentamos limitar sua flexibilidade, mas também incentivá-lo a construir soluções baseadas em vários recursos. Duas versões populares desse método são **L1 — least absolute deviations (LAD)** – e **L2 — least square errors (LSE)**. As equações que descrevem essas regularizações são dadas por:
	
	$$J_{L1}(W,b) = \frac{1}{m}\sum_{i=1}^m L(\bar{y}^{(i)}, y^{(i)}) + \lambda \parallel w \parallel_{1}, \parallel w \parallel_{1} = \sum_{j=1}^n \mid w_{j} \mid$$
	
	$$J_{L2}(W,b) = \frac{1}{m}\sum_{i=1}^m L(\bar{y}^{(i)}, y^{(i)}) + \lambda \parallel w \parallel_{2}, \parallel w \parallel_{2} = \sum_{j=1}^n w_{j}^{2}$$
	
	Na maioria dos casos, o uso de $L1$ é preferível, pois reduz a zero os valores de peso menos significativos. Além disso, $L2$ não tem um desempenho muito bom em conjuntos de dados com grande número de outliers (pontos que diferem muito dos demais dados).

* #### Dropout
	Cada unidade de nossa rede neural (exceto aquelas pertencentes à camada de saída) tem a probabilidade $p$ de ser temporariamente ignorada nos cálculos. O hiperparâmetro $p$ é chamado de **taxa de dropout** e, muitas vezes, seu valor-padrão é definido como 0,5. Então, em cada iteração, selecionamos aleatoriamente os neurônios que descartamos de acordo com a probabilidade atribuída. Como resultado, cada vez trabalhamos com uma rede neural menor.

* #### Parada antecipada ou early stopping
	Trata-se do algoritmo para quando o modelo de validação atinge um platô. A ideia é que o modelo tente seguir a função de custo durante a fase de treinamento, ajustando os parâmetros. Então, analisamos outro conjunto de dados como o conjunto de validação e, à medida que avançamos no treinamento, mantemos um registro da função de custo nos dados de validação; quando vemos que não há nenhuma melhoria no conjunto de validação, paramos, em vez de aplicarmos todas as iterações (épocas) do algoritmo de otimização.

# Reconhecer os aspectos relacionados ao aprendizado por reforço
## Aprendendo a otimizar recompensas

O aprendizado por reforço é um método de aprendizado de máquina voltado para tratar como os agentes de software devem realizar ações em um ambiente (LUGER, 2013). Ele integra os métodos de aprendizado profundo, dando suporte para maximizar recompensas cumulativas.

Nesse método, o agente não é informado por exemplos positivos e negativos.

O aprendizado é baseado em um método de tentativa e erro para encontrar as políticas ideais.

Todo o processo de aprendizagem é desenvolvido a partir de ideias em psicologia. O agente obtém recompensas a partir da interação com seu ambiente; assim, ele pode melhorar ou enfraquecer a execução de ações para obter as políticas ideais. Portanto, o objetivo final da aprendizagem por reforço é maximizar a recompensa cumulativa. A estrutura de aprendizagem é mostrada na imagem a seguir.

![](https://raw.githubusercontent.com/joaomarcosrs/study-notes/main/Machine%20Learning%20-%20Wyden/static/img_04_01.jpg "Imagem: Sérgio Assunção Monteiro")

Estrutura de aprendizagem por reforço

Vamos explicar um pouco mais sobre os principais elementos da aprendizagem por reforço que estão representados na imagem.

* #### Agente
	É uma entidade que realiza ações em um ambiente para ganhar alguma recompensa.
* #### Ambiente
	É o cenário em que um agente está contextualizado, ou seja, onde realiza ações e aprende.
* #### Estado (e)
	É uma situação concreta e imediata em que o agente se encontra. Por exemplo, o agente pode estar em um local e em momento específicos. Trata-se de uma configuração instantânea que coloca o agente em relação a outras coisas significativas, como ferramentas, obstáculos, inimigos ou prêmios.
* #### Recompensa (r)
	É o retorno dado para o agente como modo de premiar o seu sucesso ou penalizar o seu fracasso, ao realizar ações em determinado estado. Por exemplo, em um jogo, quando um personagem atinge um objetivo, ele ganha pontos. A partir de qualquer estado, um agente realiza ações no ambiente e, por sua vez, o ambiente retorna o novo estado do agente, bem como sua recompensa. As recompensas podem ser imediatas ou, ainda, atrasadas. Elas são o modo como as ações do agente são avaliadas efetivamente.
* #### Política (π)
	É uma estratégia aplicada pelo agente para decidir a próxima ação com base no estado atual. Ela faz o mapeamento dos estados nas ações com maiores recompensas.
* #### Valor (V)
	É definido como o retorno esperado a longo prazo do estado atual em relação à política $\pi$. Quanto mais longo for o tempo para se obter uma recompensa, maior será o desconto do seu valor aplicado, ou seja, o seu valor estimado é reduzido.
* #### Função valor
	Ela mede qual é o valor de um estado, isto é, calcula o valor total da recompensa. É essa função que o agente tem como objetivo maximizar.
* #### Valor Q
	É bastante semelhante ao valor. A diferença entre os dois é que o valor $Q$ leva em consideração o estado, a ação e a política $\pi$ adotada pelo agente, ou seja, trata do valor da recompensa de um agente em relação aos pares de estado-ação para determinada política. Normalmente, ela é representada por $Q^{\pi}(e, a)$.

A ideia da otimização de recompensas, portanto, consiste no mapeamento de ações a valores associadas a políticas contextualizadas no ambiente em que o agente vai atuar.
## Políticas de rede neural
Como vimos, uma política de rede neural é uma regra usada pelo agente para decidir o que fazer, dado o conhecimento do estado atual do ambiente.

A política é definida matematicamente (HAYKIN, 2008) por:

$$\pi = \{\micro_{0}, \micro_{1}, \micro_{2}, \dots\}.$$

Sendo que $\micro_{t}$ é uma função que mapeia o estado $E_{t} = e$ em uma ação $A_{t} = a$ no instante $t$.

Esse mapeamento é tal que:

$$\micro_{t}(e) \in A_{e}, \forall\space e \in E.$$

Onde $A_{e}$ representa o conjunto de todas as ações possíveis tomadas pelo agente no estado $e$.

As políticas organizadas de forma a serem utilizadas pelas redes neurais são chamadas de políticas admissíveis. Uma política pode ser **não estacionária** ou **estacionária**:

* #### Política não estacionária
	Varia com o tempo. Em termos matemáticos, isso é dado exatamente como representamos o conjunto de elementos de uma política, com µt variando ao longo do tempo.
* #### Política estacionária
	É independente do tempo. Em outras palavras, uma política estacionária especifica exatamente a mesma ação cada vez que ocorre determinado estado.

Basicamente, existem duas categorias principais para a otimização da política na aprendizagem por reforço: os **métodos baseados em valor** e os **métodos baseados em políticas**. Existe ainda uma classe de algoritmos conhecida como **ator-crítico**, que é uma combinação dessas duas categorias, o que potencializa a função valor para atualizar a política.

### Métodos baseados em valor

Geralmente, implicam otimização da função valor $Q^{\pi*}(e,a) = max_{a}Q^{\pi}(e,a)$. A otimização da função vai encontrar a política que gera o melhor retorno. Isso é representado pela notação:

Onde o $*$ representa a solução ótima. Como nesses algoritmos trabalhamos com soluções aproximadas, a política ótima pode ser representada por:

$$\pi^{*} \approx \arg max_{\pi}Q^{\pi}$$

Onde o símbolo $\approx$ é usado para representar o erro de aproximação.

Os algoritmos mais comuns baseados em valores são:

* #### Q-learning
* #### DQN

As vantagens dessa categoria de algoritmos é que normalmente eles são mais eficientes em termos de amostra do que os algoritmos baseados em políticas. Isso ocorre porque eles têm menor variância e fazem melhor uso dos dados coletados do ambiente.

> No entanto, não há garantias de que esses algoritmos irão convergir para uma solução ótima. Em sua formulação-padrão, eles também são aplicáveis apenas a ambientes com espaços de ação discretos.
### Método baseado em política
Ele faz a otimização através de um processo de atualização da política de modo iterativo até que o retorno cumulativo seja maximizado. A ideia é que, se um agente precisa agir em um ambiente, faz sentido aprender uma política. O que constitui uma boa ação em determinado momento depende do estado; portanto, uma função de política toma um estado $e$ como entrada para produzir uma ação $a$.

* #### Vantagem
	Fazem parte de uma classe muito geral de métodos de otimização e podem ser aplicados a problemas com qualquer tipo de ação, discreta, contínua ou mista. Além disso, essa classe de métodos converge localmente para uma política ótima.
* #### Desvantagem
	Têm alta variância e são ineficientes para a amostra.

Alguns algoritmos baseados em políticas comuns incluem gradiente de política, região de confiança e evolução.

## Aprendizado por diferenças temporais e Q-learning

Para podermos entender o método de aprendizado por diferença temporal, precisamos compreender as funções valor (ou funções de valor). Elas são funções que recebem como entrada o par “estado e ação” e, assim, estimam quão boa uma ação particular será em determinado estado, ou seja, qual será o retorno para aquela ação. Elas são representadas matematicamente por:

* $V^{\pi}(e)$
	É o valor esperado de um estado e em relação a política $\pi$.

* $Q^{\pi}(e,a)$
	É o valor esperado ao iniciar a ação a no estado e seguindo a política $\pi$.

O problema em questão é estimar essas funções valor para uma política específica. O nosso objetivo é estimar essas funções de valor para que possam ser usadas para escolher uma ação que fornecerá a melhor recompensa total possível, depois de estar naquele determinado estado.

Os métodos de aprendizado por diferença temporal são usados para estimar essas funções valor. Se não fosse feita a estimativa das funções valor, o agente precisaria esperar até que a recompensa final fosse recebida antes que qualquer valor do par estado-ação pudesse ser atualizado. Assim que a recompensa final fosse recebida, o caminho percorrido para chegar ao estado final precisaria ser rastreado e cada valor atualizado de acordo. Isso pode ser expresso formalmente como (HAYKIN, 2008):

$$V(E_{t}) \leftarrow V(E_{t}) + \alpha[R_{t} - V(E_{t})]$$

Onde $E_{t}$ é o estado no instante $t$, $R_{t}$ é a recompensa no instante $t$ e $\alpha$ é um parâmetro constante.

Nos métodos de aprendizado por diferença temporal, uma estimativa da recompensa final é calculada em cada estado, e o valor da ação do estado é atualizado para cada etapa do caminho. A fórmula, portanto, é expressa como:

$$V(E_{t}) \leftarrow V(E_{t}) + \alpha[R_{t+1} + \gamma V(E_{t+1} - V(E_{t}))]$$

Onde $E_{t+1}$ é a recompensa observada no instante $t + 1$.

> Uma importante distinção entre algoritmos de aprendizado por reforço é se eles estão dentro ou fora da política. Essa característica determina como as iterações de treinamento usam os dados.

* #### Algoritmo dentro da política
	Um algoritmo está **dentro da política** se aprender sobre a política – ou seja, o treinamento só pode utilizar dados gerados a partir da política atual. Isso significa que, conforme o treinamento avança por meio de versões de políticas, $\micro_{1}, \micro_{2}, \micro_{3}, \dots$, cada iteração de treinamento usa apenas a política atual naquele momento para gerar dados de treinamento. A consequência é que todos os dados devem ser descartados após o treinamento, uma vez que se tornam inutilizáveis. Isso torna os métodos dentro da política ineficientes para a amostragem – eles exigem mais dados de treinamento.

* #### Algoritmo fora da política
	No caso dos algoritmos que estão **fora da política**, quaisquer dados coletados podem ser reutilizados no treinamento. Consequentemente, os métodos fora da política são mais eficientes na amostragem, mas isso pode exigir muito mais memória para armazenar os dados.

> **Atenção**
> Os métodos de diferença temporal estão dentro da política, ou seja, aprendem o valor da política que é usada para tomar decisões.

As funções de valor são atualizadas usando-se os resultados da execução de ações determinadas por alguma política. Essas políticas são geralmente suaves e não determinísticas. O significado de suave, nesse sentido, é que ele garante que sempre haja um elemento de exploração na política. A política não é tão rígida a ponto de sempre escolher a ação que mais recompensa. As políticas mais comuns são:

*  $\varepsilon$-soft
*  $\varepsilon$-greedy
*  Softmax

Os métodos fora da política podem aprender diferentes políticas de comportamento e estimativa. Neles, a política de comportamento é geralmente suave. Algoritmos fora da política podem atualizar as funções de valor estimado usando ações hipotéticas, aquelas que não foram realmente tentadas. Isso contrasta com os métodos dentro da política, que atualizam funções de valor com base estritamente na experiência. Isso significa que algoritmos fora da política podem separar a exploração do controle, e os algoritmos dentro da política não. Em outras palavras, um agente treinado usando um método fora da política pode acabar aprendendo táticas que não necessariamente exibiu durante a fase de aprendizado.

O **Q-learning** é um método de aprendizado por reforço que usa Q-valores para melhorar iterativamente o comportamento do agente no processo de aprendizado. Ele foi apresentado pela primeira vez em Watkins (1989).

> **Você sabia**
> A letra Q no Q-learning não significa nada de especial; é apenas uma notação que foi adotada por Watkins em sua derivação original do algoritmo. Trata-se de um método fora da política.

A tarefa comportamental desse sistema é encontrar uma política ótima (ou seja, custo mínimo) depois de tentar várias sequências de ações e observar os custos obtidos e as transições de estado que ocorrem. Esse algoritmo trabalha com duas políticas: a de comportamento e a de estimativa. A primeira é voltada para o comportamento do agente, enquanto a segunda tem como objetivo estimar o valor de uma política.

A função valor do Q-learning pode ser representada da seguinte forma (HAYKIN, 2008):

$$Q(E_{t}, A_{t}) \leftarrow Q(E_{t}, A_{t}) + \alpha[R_{t} + \gamma \max_{a} Q(E_{t+1}, a) - Q(E_{t}, A_{t})]$$
Essa regra de atualização para estimar o valor de $Q$ é aplicada em cada etapa de iteração dos agentes com o ambiente. Vamos detalhar um pouco mais sobre os termos usados na função:

$E_{t}$, representa o estado do agente no instante

$E_{t+1}$, representa o próximo estado do agente

$A_{t+1}$, é a próxima melhor ação a ser escolhida usando-se a estimativa do valor atual de Q, ou seja, é a ação com o valor Q máximo do próximo estado

$R_{t}$, recompensa atual observada do ambiente em resposta à ação atual $A_{t}$

$\gamma$, é o fator de desconto para recompensas futuras. É um valor real entre 0 e 1

$\alpha$, é o comprimento do passo levado para atualizar a estimativa de $Q(E_{t}, A_{t})$

A seguir, apresentamos o algoritmo Q-learning:

* ##### Iniciar
	$\space\space\space\space\space\space\space\space Q(E_{0}, A_{0})$
	
* ##### Para  $k = 1,2,3,\dots$
	$\space\space\space\space\space\space\space\space Iniciar \space o \space estado \space\space\space e$
	
	$\space\space\space\space\space\space\space\space Repetir$
	
	$\space\space\space\space\space\space\space\space Escolher \space uma \space ação \space a \space do \space estado \space e \space usando \space uma \space política \space derivada \space de \space\space Q$
	
	$\space\space\space\space\space\space\space\space Calcule\space\space\space\space (E_{t}, A_{t}) \leftarrow Q(E_{t}, A_{t}) + \alpha[R_{t} + \gamma \max_{a} Q(E_{t+1}, a) - Q(E_{t}, A_{t})]$
	
	$\space\space\space\space\space\space\space\space Atualizar \space o \space estado\space\space\space\space E_{t} \leftarrow E_{t+1}$
	
* ##### Até que
	$\space\space\space\space\space\space\space E_{t} \space seja \space um \space estado \space terminal$

A política a que se refere o algoritmo Q-learning pode ser a política ε-gananciosa, que é uma política muito simples de escolha de ações usando as estimativas de valor Q atuais. Basicamente, ela funciona assim:

* ##### Com probabilidade $(1 - \varepsilon)$
	O algoritmo escolhe a ação que tem o valor Q mais alto.

* ##### Com probabilidade $(\varepsilon)$
	O algoritmo escolhe qualquer ação aleatoriamente.

## Utilizando o algoritmo DQN

Para a maioria das aplicações, não é viável representar a função valor

como uma tabela contendo os valores para todas as possíveis combinações de ações e estados. Para tratar desses casos, foi desenvolvido o método Deep Q-Network. O DQN é um método de aprendizagem de reforço e fora da política. Ele é uma variante do Q-learning. Algumas das técnicas mais comuns que o DQN utiliza para resolver problemas relacionados à instabilidade do aprendizado são as seguintes:

* #### Repetir a experiência (experience replay)
	A ideia básica é que, ao armazenar as experiências de um agente e, em seguida, projetar lotes aleatoriamente delas para treinar a rede, podemos aprender de forma mais robusta e ter um bom desempenho na tarefa. Ao manter as experiências que extraímos aleatoriamente, evitamos que a rede apenas aprenda sobre o que está fazendo imediatamente no ambiente e permitimos que ela aprenda com uma gama mais variada de experiências anteriores. Cada uma dessas experiências é armazenada como uma tupla de <estado, ação, recompensa, próximo estado>. O buffer do experience replay armazena um número fixo de memórias recentes e, conforme as novas entram, as antigas são removidas. Quando chega a hora de treinar, simplesmente extraímos um lote de memórias aleatórias do buffer e treinamos nossa rede com elas.

* #### Rede alvo (target network)
	O DQN usa outra rede neural chamada rede de destino para obter um estimador do erro quadrático médio usado no treinamento da rede Q. A rede de destino é sincronizada com a rede Q após certo período de iterações.

Agora, apresentamos o algoritmo DQN (HAYKIN, 2008):
* ##### Iniciar $Q(E_{0}, A_{0})$ com valores aleatórios para os pesos $\theta$
* ##### Iniciar a memória $B$ com o método Experience Replay
* ##### Iniciar o procedimento de exploração $Explorar (.)$
* ##### Para  $i = 1,2,\dots,N$ faça
	
	$\space\space\space\space\space\space\space\space y^{i}_{e,a} = \mathbb{E}_{B} [r + max_{a}Q, (e_{t+1}, a_{t+1};\theta_{i-1})\mid e,a]$
	
	$\space\space\space\space\space\space\space\space \theta \approx \arg min_{\theta}E_{B}[(y^{i}_{e,a} - Q(e_{t}, a_{t};\theta))^{2}]$
	
	$\space\space\space\space\space\space\space\space Explorar(.), \space Atualizar \space B$
	
	$\space\space\space\space\space\space\space\space Fim$
	
* ##### Saída $Q^{DQN}(e, a, \theta_{N})$

Na linha 5, os valores alvo $y^{i}_{e,a}$ são construídos usando uma rede-alvo designada $Q(e_{t+1}, a_{t+1};\theta_{i-1})$ com o uso dos parâmetros de iteração anteriores $\theta_{i-1}$, em que o retorno esperado $\mathbb{E}_{B}$ é obtido a partir de uma amostra da distribuição de transições de experiência no buffer $(e_{t},a_{t};\theta)\space B$ . Na linha 6, o algoritmo DQN resolve um problema de aprendizado supervisionado para aproximar a função valor $Q(e_{t},a_{t};\theta)$. A perda do DQN é minimizada usando-se uma variação do algoritmo de gradiente de descida estocástica. Além disso, o DQN utiliza um procedimento de exploração para interagir com o ambiente – por exemplo, um procedimento de exploração gananciosa que chamamos de $Explorar(.)$.

# Identificar as redes neurais de convolução (CNN) e recorrentes (RNN)
# Esquematizar simulação com TensorFlow e AWS SageMaker